# CatBoost_Gini_Based_Confidence_Estimates
A method to deduce important decision rules from the CatBoost model architecture

Dataset and classification task:
University of Southern Denmark, 2023 Exam collaboration with Udviklings- og Forenklingsstyrelsen (UFST). Classification task to catch Tax Fraudulent businesses in Denmark. Dataset is anonymized, values are distorted, outliers removed and variable names anonymous. Dataset consists of 240 features, 2 classes. "Not tax fraudulent"=0, "Tax fraudulent"=1. Any data preparation in this repositary, is based solely on a statistical basis, not on industry knowledge. 

Can we consider the model competent in its ability to evaluate what is truly important for the task? 
The danger with feature based importances, SHAP values and so forth, is that we implicitly expect the model to be "competent". If we consider the initial classification results (see the ipynb file) one would hesitate to answer yes. It clearly does a good job predicting not fraudulent, but not so much for the fraudulent. 
But even if the model was extremely talented, I would still hesitate to call it "competent". Consider for a minute the thousands or even millions of models, of which many have tried to be implemented, but eventually failed. A great deal of those models certainly had SOTA performance, but once applied to the real world, it falls apart. Bad data management, distribution shift, concept drift etc. The world is tough. Truly it is only a small fraction of models we can call competent, and that is often only proven when a model is in production, and it is performing well. Once you realise it is a statistical improbability that the model is competent, you can not rely on feature based importance scores alone, even SHAP values, to decide whether a feature is important or not.  

Ideally, we would traverse the CatBoost model architecture, calculate the gini impurities of the leaf nodes, and then extract the associated feature splits that leads to the purest nodes. CatBoost does not provide scikit-like functionality that more easily enables one to do this. 

So, I propose a compromise. We use the few functionalities CatBoost does provide: calculate_leaf_indexes, this returns the exact leaf node each prediction i belongs to, after j-th iteration. Hence, we can access the final leaf node, calculate the class distribution and then calculate the gini impurities for the final leaf nodes. A compromise to extracting the exact feature splits in the tree of the pure nodes, then, is to use SHAP decision plots to see which features had the most impact on the most pure nodes, as given by our calculation of gini impurities. This should help us identify which features are associated with the highest quality decision rules. As one will see, by using this method, I get to debunk a formerly strong held opinion of mine, that feature ejerkreds_aktiv_86 is important for the classification task. 
